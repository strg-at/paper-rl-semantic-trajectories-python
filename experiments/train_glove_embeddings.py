import argparse

import tinygrad as tg
import torch
from tqdm import tqdm

from rl_semantic_trajectories import models
from rl_semantic_trajectories.dataset.load_cooccurrence_dataset import (
    cooccurrence_iterator,
)


def get_iterator(cooccurr_file: str, batch_size: int):
    return cooccurrence_iterator(cooccurr_file, batch_size)


def train_with_pytorch(args, vocab):
    device = torch.device("cpu")
    if torch.cuda.is_available():
        device = torch.device("cuda")

    glove = models.GloVe(len(vocab), args.embeddings_size, args.x_max, args.alpha).to(device)

    optimizer = torch.optim.Adagrad(glove.parameters(), lr=args.learning_rate)

    losses = []
    pbar = tqdm(range(args.epochs), desc="Training GloVe", total=args.epochs, position=0)

    for epoch in pbar:
        epoch_loss = 0
        n_batches = 0
        batch_iterator = get_iterator(args.cooccurr_file, args.batch_size)
        for i, (w1, w2, x) in tqdm(
            enumerate(batch_iterator),
            desc=f"Epoch {epoch}",
            position=1,
        ):
            w1_pt = torch.tensor(w1, device=device)
            w2_pt = torch.tensor(w2, device=device)
            x_pt = torch.tensor(x, device=device)
            optimizer.zero_grad()
            loss = glove(w1_pt, w2_pt, x_pt)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.detach().item()
            n_batches += 1
        losses.append(epoch_loss)
        pbar.set_postfix({"loss": epoch_loss / n_batches})

    torch.save(glove.state_dict(), args.embs_save_path)
    with torch.no_grad():
        idxs = torch.arange(len(vocab), device=device)
        embeddings = glove.weight(idxs).cpu().numpy()
        return embeddings


def train_with_tinygrad(args, vocab):
    glove = models.GloVeTG(len(vocab), args.embeddings_size, args.x_max, args.alpha)
    optimizer = tg.nn.optim.Adam(tg.nn.state.get_parameters(glove), lr=1e-3)
    losses = []
    pbar = tqdm(range(args.epochs), desc="Training GloVe", total=args.epochs, position=0)

    @tg.TinyJit
    def train_step(w1, w2, x):
        with tg.Tensor.train():
            w1_tg = tg.Tensor(w1)
            w2_tg = tg.Tensor(w2)
            x_tg = tg.Tensor(x)

            optimizer.zero_grad()
            loss = glove(w1_tg, w2_tg, x_tg)
            loss.backward()
            optimizer.step()
            return loss

    for epoch in pbar:
        epoch_loss = 0
        batch_iterator = get_iterator(args.cooccurr_file, args.batch_size)
        n_batches = 0
        for i, (w1, w2, x) in tqdm(
            enumerate(batch_iterator),
            desc=f"Epoch {epoch}",
            position=1,
        ):
            loss = train_step(w1, w2, x)
            epoch_loss += loss.item()
        losses.append(epoch_loss / n_batches)
        pbar.set_postfix({"loss": epoch_loss})

    state_dict = tg.nn.state.get_state_dict(glove)
    tg.nn.state.safe_save(state_dict, args.embs_save_path)
    with tg.Tensor.train(False):
        idxs = tg.Tensor.arange(len(vocab))
        embeddings = glove.weight(idxs).numpy()
        return embeddings


if __name__ == "__main__":
    desc = """\
    Train GloVe embeddings using trajectories as tokens. This code only deals with training the model, given
    the `vocab.txt` and a cooccurence file generated by the original GloVe C programs.
    The original GloVe is provided as a submodule (if you cloned with `git clone --recursive`): in order to generate
    the necessary files, simply run the `demo.sh` in the `submodules/GloVe` directory.
    """.strip()
    parser = argparse.ArgumentParser("Train GloVe embeddings", description="")
    parser.add_argument("--vocab-file", type=str, required=True, help="Path to vocab file")
    parser.add_argument("--cooccurr-file", type=str, required=True, help="Path to cooccurences file")
    parser.add_argument("--embeddings-size", type=int, default=100, help="Embeddings size")
    parser.add_argument("--embs-save-path", type=str, required=True, help="Path to save embeddings")
    parser.add_argument(
        "--glove-vectors-save-path",
        type=str,
        required=True,
        help="Path to save GloVe vectors in GloVe vectors.txt format (see original repo).",
    )
    parser.add_argument("--batch-size", type=int, default=1000, help="Batch size for training")
    parser.add_argument("--learning-rate", type=float, default=0.05, help="Learning rate")
    parser.add_argument("--epochs", type=int, default=100, help="Number of epochs to train")
    parser.add_argument("--x-max", type=int, default=100, help="GloVe X max value")
    parser.add_argument("--alpha", type=float, default=0.75, help="GloVe X max value")
    parser.add_argument("--use-tinygrad", action="store_true", help="Use tinygrad instead of pytorch")

    args = parser.parse_args()

    vocab = {}
    with open(args.vocab_file, "r") as f:
        for i, line in tqdm(enumerate(f), desc="Reading vocab file"):
            word, _ = line.strip().split()
            vocab[word] = i

    if args.use_tinygrad:
        embs = train_with_tinygrad(args, vocab)
    else:
        embs = train_with_pytorch(args, vocab)

    with open(args.glove_vectors_save_path, "w") as f:
        for word, idx in vocab.items():
            emb = embs[idx]
            emb_str = " ".join(str(x) for x in emb)
            f.write(f"{word} {emb_str}\n")
